import json
import re
from pathlib import Path
import pytest
from main import extract_scenarios_from_pdf

# --- Test Configuration ---
# This is the primary PDF used for generating and comparing snapshots.
# To test against a new season's file, change this path.
PDF_PATH = Path(__file__).parent / "its-rules-season-17-en-v1.0.1.pdf"

# The output file that will be generated by the test run.
OUTPUT_JSON_PATH = Path(__file__).parent / "its_scenarios.json"

# --- Fixtures ---

@pytest.fixture(scope="session")
def extracted_data():
    """
    Runs the PDF extraction once per test session and returns the results.
    This fixture ensures the main `its_scenarios.json` is created before
    any tests that depend on it are parameterized.
    """
    if not PDF_PATH.exists():
        pytest.fail(
            f"Test PDF file not found at {PDF_PATH}. "
            f"Please ensure '{PDF_PATH.name}' is in the project root."
        )

    # 1. Extract scenarios from the PDF
    scenarios = extract_scenarios_from_pdf(str(PDF_PATH), debug=False)

    # 2. Extract season and version from the filename
    season = "Unknown"
    version = "N/A"
    match = re.search(r'its-rules-season-(\d+)-en-(v[\d\.]+)\.pdf', PDF_PATH.name, re.IGNORECASE)
    if match:
        season = f"Season {match.group(1)}"
        version = match.group(2)

    # 3. Assemble the full output data structure
    output_data = {
        "season": season,
        "version": version,
        "scenarios": scenarios
    }

    # 4. Write the data to the JSON file, so other tests or manual inspection can use it
    with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)

    return output_data


@pytest.fixture(scope="session")
def scenarios_for_parametrization(extracted_data):
    """
    Provides the list of scenarios to be used in parameterized tests.
    Depends on `extracted_data` to ensure the JSON file is created first.
    """
    return extracted_data.get("scenarios", [])


# --- Snapshot Tests ---

def test_full_output_snapshot(snapshot, extracted_data):
    """
    Tests the entire generated JSON output against a single, comprehensive snapshot.
    This is good for catching high-level changes or modifications to the overall structure.
    """
    # Create a clean, sorted JSON string for stable snapshot comparison
    snapshot_content = json.dumps(extracted_data, indent=2, sort_keys=True, ensure_ascii=False)
    snapshot.assert_match(snapshot_content, "full_output_snapshot.json")


@pytest.mark.parametrize(
    "scenario",
    [
        pytest.param(
            s,
            id=s.get("name", f"scenario_{i}").replace(" ", "_").replace("/", "_")
        )
        for i, s in enumerate(json.load(open(OUTPUT_JSON_PATH, "r", encoding="utf-8")).get("scenarios", []))
    ]
)
def test_each_scenario_snapshot(snapshot, scenario):
    """
    Tests each individual scenario against its own dedicated snapshot file.
    This makes it easy to identify regressions in specific scenarios.
    
    The test name is sanitized to create a valid file name for the snapshot.
    """
    scenario_name = scenario.get("name", "Unknown_Scenario").replace(" ", "_").replace("/", "_")
    
    # Create a clean, sorted JSON string for stable snapshot comparison
    snapshot_content = json.dumps(scenario, indent=2, sort_keys=True, ensure_ascii=False)
    snapshot.assert_match(snapshot_content, f"{scenario_name}_snapshot.json")
